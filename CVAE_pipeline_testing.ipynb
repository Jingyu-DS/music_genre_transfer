{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aec28f73-11a1-4621-a09e-261f991a23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoading import DataLoad\n",
    "# Test DataLoading\n",
    "data_path = \"./Data/images_original\"\n",
    "data_loader = DataLoad(data_path)\n",
    "X, y = data_loader.fetch_dataset(dx=0, dy=0, dimx=128, dimy=128)\n",
    "genre_names = data_loader.get_genre_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "089d8908-e197-478c-b7c4-af6f581a5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from DataLoading import DataLoad\n",
    "from EncoderCVAE import CVAE_Encoder\n",
    "from DecoderCVAE import CVAE_Decoder\n",
    "from Loss import get_loss\n",
    "from CVAETrainerConstruct import TrainerCVAE\n",
    "from datetime import datetime\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b414bc5-fbd0-48bd-ba40-e81d4f572d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        # Here label is an integer. For the CVAE, we want a condition vector.\n",
    "        num_classes = 10\n",
    "        condition = torch.nn.functional.one_hot(torch.tensor(label), num_classes=num_classes).float()\n",
    "        return img, condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "328f9f5d-4ee0-4167-af5d-961622a83bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    DATASET_PATH = \"./Data/images_original\"\n",
    "    dx, dy = 0, 0\n",
    "    dim = 128\n",
    "    batch_size = 64\n",
    "\n",
    "    dataload = DataLoad(DATASET_PATH)\n",
    "    all_photos, all_labels = dataload.fetch_dataset(dx, dy, dim, dim)\n",
    "    \n",
    "    # Normalize images by converting from uint8 [0, 255] to float [0, 1].\n",
    "    # Note: transforms.ToTensor() does the conversion and also reorders dimensions to (C, H, W).\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Split data into training and validation sets.\n",
    "    X_train, X_val, y_train, y_val = train_test_split(all_photos, all_labels, test_size=0.1, random_state=365)\n",
    "    \n",
    "    train_dataset = GenreDataset(X_train, y_train, transform=transform)\n",
    "    val_dataset = GenreDataset(X_val, y_val, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36b5e508-3a41-4a59-b8dc-5360dcdc3ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 66204.4737, Test Loss: 35487.7109\n",
      "Epoch [2/50], Train Loss: 18599.9132, Test Loss: 11320.0312\n",
      "Epoch [3/50], Train Loss: 8128.0350, Test Loss: 7815.2983\n",
      "Epoch [4/50], Train Loss: 5873.3525, Test Loss: 10292.7354\n",
      "Epoch [5/50], Train Loss: 5079.1071, Test Loss: 17583.8945\n",
      "Epoch [6/50], Train Loss: 4291.8943, Test Loss: 4350.2566\n",
      "Epoch [7/50], Train Loss: 3897.8581, Test Loss: 7825.2402\n",
      "Epoch [8/50], Train Loss: 3679.6737, Test Loss: 3954.7218\n",
      "Epoch [9/50], Train Loss: 3739.7708, Test Loss: 8792.3535\n",
      "Epoch [10/50], Train Loss: 4007.2799, Test Loss: 5630.2461\n",
      "Epoch [11/50], Train Loss: 3403.7545, Test Loss: 8118.7983\n",
      "Epoch [12/50], Train Loss: 3249.3814, Test Loss: 3697.3263\n",
      "Epoch [13/50], Train Loss: 3138.9090, Test Loss: 3744.4889\n",
      "Epoch [14/50], Train Loss: 3055.0602, Test Loss: 6068.6685\n",
      "Epoch [15/50], Train Loss: 3185.0572, Test Loss: 4247.1689\n",
      "Epoch [16/50], Train Loss: 3043.8439, Test Loss: 4100.9945\n",
      "Epoch [17/50], Train Loss: 2969.6236, Test Loss: 3267.2416\n",
      "Epoch [18/50], Train Loss: 2999.1007, Test Loss: 3079.8033\n",
      "Epoch [19/50], Train Loss: 2838.0103, Test Loss: 3359.8491\n",
      "Epoch [20/50], Train Loss: 2837.6506, Test Loss: 3921.9645\n",
      "Epoch [21/50], Train Loss: 3015.9045, Test Loss: 7050.9417\n",
      "Epoch [22/50], Train Loss: 3937.4269, Test Loss: 8015.2327\n",
      "Epoch [23/50], Train Loss: 3091.3398, Test Loss: 9281.4546\n",
      "Epoch [24/50], Train Loss: 2991.8321, Test Loss: 3239.1891\n",
      "Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [25/50], Train Loss: 3014.4453, Test Loss: 7178.8635\n",
      "Epoch [26/50], Train Loss: 3012.8347, Test Loss: 2949.1879\n",
      "Epoch [27/50], Train Loss: 2785.5003, Test Loss: 3390.9652\n",
      "Epoch [28/50], Train Loss: 2742.0702, Test Loss: 2839.7963\n",
      "Epoch [29/50], Train Loss: 2661.5690, Test Loss: 3069.7164\n",
      "Epoch [30/50], Train Loss: 2685.3958, Test Loss: 3085.3975\n",
      "Epoch [31/50], Train Loss: 2654.0016, Test Loss: 3030.9443\n",
      "Epoch [32/50], Train Loss: 2648.4763, Test Loss: 2747.7609\n",
      "Epoch [33/50], Train Loss: 2636.4052, Test Loss: 2755.4662\n",
      "Epoch [34/50], Train Loss: 2643.9235, Test Loss: 2998.0078\n",
      "Epoch [35/50], Train Loss: 2607.8657, Test Loss: 2699.3595\n",
      "Epoch [36/50], Train Loss: 2596.3740, Test Loss: 2667.2130\n",
      "Epoch [37/50], Train Loss: 2594.9768, Test Loss: 2744.2626\n",
      "Epoch [38/50], Train Loss: 2616.0746, Test Loss: 3186.7615\n",
      "Epoch [39/50], Train Loss: 2584.9063, Test Loss: 2741.0708\n",
      "Epoch [40/50], Train Loss: 2625.3436, Test Loss: 2810.0012\n",
      "Epoch [41/50], Train Loss: 2566.0051, Test Loss: 2652.1365\n",
      "Epoch [42/50], Train Loss: 2598.2835, Test Loss: 2702.3546\n",
      "Epoch [43/50], Train Loss: 2486.7344, Test Loss: 2806.7805\n",
      "Epoch [44/50], Train Loss: 2530.8723, Test Loss: 2714.7279\n",
      "Epoch [45/50], Train Loss: 2634.3560, Test Loss: 2809.3325\n",
      "Epoch [46/50], Train Loss: 2527.2285, Test Loss: 2622.6755\n",
      "Epoch [47/50], Train Loss: 2487.3303, Test Loss: 2793.7118\n",
      "Epoch [48/50], Train Loss: 2578.0609, Test Loss: 2661.1411\n",
      "Epoch [49/50], Train Loss: 2500.5949, Test Loss: 2610.5974\n",
      "Epoch [50/50], Train Loss: 2443.0120, Test Loss: 2612.5847\n",
      "Training complete!\n",
      "Models saved as cvae_encoder_20250411_154917.pth and cvae_decoder_20250411_154917.pth\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    LATENT_SPACE_SIZE = 128\n",
    "    CONDITION_DIM = 10\n",
    "    \n",
    "    train_loader, test_loader = prepare_data()\n",
    "    \n",
    "    encoder = CVAE_Encoder(latent_dim=LATENT_SPACE_SIZE, condition_dim=CONDITION_DIM, input_shape=(3, 128, 128), use_embedding=False)\n",
    "    decoder = CVAE_Decoder(latent_dim=LATENT_SPACE_SIZE, condition_dim=CONDITION_DIM)\n",
    "    \n",
    "    trainer = TrainerCVAE(\n",
    "        trainloader=train_loader,\n",
    "        testloader=test_loader,\n",
    "        Encoder=encoder,\n",
    "        Decoder=decoder,\n",
    "        latent_dim=LATENT_SPACE_SIZE,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    trainer.train(num_epochs=50, factor=10)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    torch.save(encoder.state_dict(), f\"cvae_encoder_{timestamp}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"cvae_decoder_{timestamp}.pth\")\n",
    "    print(f\"Models saved as cvae_encoder_{timestamp}.pth and cvae_decoder_{timestamp}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78acdb-b008-4b67-946f-9e0a57b3f133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2150c44-1f6b-4b80-9dcd-4b0346599375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AudioDataLoading import DataLoad\n",
    "\n",
    "# Remove './Data/genres_original/jazz/jazz.00054.wav' from the data folder as this file is broken and it will crash the loader\n",
    "data_path = \"./Data/genres_original\"\n",
    "data_loader = DataLoad(data_path)\n",
    "X, y = data_loader.fetch_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14deee6-6de8-4756-88c0-adcbbcfcae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 38762054376083.9609, Test Loss: 254000.5312\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from AudioDataLoading import DataLoad\n",
    "from AudioCVAEEncoderArch import AudioCVAEEncoder  \n",
    "from AudioCVAEDecoderArch import AudioCVAEDecoder  \n",
    "from Loss import get_loss\n",
    "from AudioCVAETrainer import TrainerCVAE\n",
    "from datetime import datetime\n",
    "\n",
    "def prepare_data():\n",
    "    DATASET_PATH = \"./Data/genres_original\"\n",
    "    batch_size = 32\n",
    "    \n",
    "    dataload = DataLoad(DATASET_PATH)\n",
    "    all_audios, all_attrs = dataload.fetch_dataset()  \n",
    "    # Convert to tensors.\n",
    "    all_audios = torch.from_numpy(all_audios).float()\n",
    "    all_attrs = torch.from_numpy(all_attrs).long()\n",
    "    \n",
    "    # Add a channel dimension: (N, 1, T)\n",
    "    all_audios = all_audios.unsqueeze(1)\n",
    "    \n",
    "    # Split both audio and attributes\n",
    "    X_train, X_val, y_train, y_val = train_test_split(all_audios, all_attrs, test_size=0.1, random_state=365)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def main():\n",
    "    LATENT_SPACE_SIZE = 128\n",
    "    CONDITION_DIM = 10\n",
    "    INPUT_LENGTH = 661500\n",
    "    OUTPUT_LENGTH = 661500\n",
    "    \n",
    "    train_loader, test_loader = prepare_data()\n",
    "    \n",
    "    # Instantiate CVAE models\n",
    "    encoder = AudioCVAEEncoder(\n",
    "        latent_dim=LATENT_SPACE_SIZE,\n",
    "        input_length=INPUT_LENGTH,\n",
    "        condition_dim=CONDITION_DIM,\n",
    "        use_embedding=False\n",
    "    )\n",
    "    decoder = AudioCVAEDecoder(\n",
    "        latent_dim=LATENT_SPACE_SIZE,\n",
    "        condition_dim=CONDITION_DIM,\n",
    "        output_length=OUTPUT_LENGTH\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer for CVAE. \n",
    "    # each batch is a tuple (audio, genre), and you convert the genre to a one-hot vector.\n",
    "    trainer = TrainerCVAE(\n",
    "        trainloader=train_loader,\n",
    "        testloader=test_loader,\n",
    "        Encoder=encoder,\n",
    "        Decoder=decoder,\n",
    "        latent_dim=LATENT_SPACE_SIZE,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    trainer.train(num_epochs=50, factor=10)\n",
    "    \n",
    "    timestamp = get_timestamp()\n",
    "    encoder_name = f\"audio_cvae_encoder_{timestamp}.pth\"\n",
    "    decoder_name = f\"audio_cvae_decoder_{timestamp}.pth\"\n",
    "    \n",
    "    torch.save(encoder.state_dict(), encoder_name)\n",
    "    torch.save(decoder.state_dict(), decoder_name)\n",
    "    print(f\"Models saved as {encoder_name} and {decoder_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fec2bc-569b-46c8-a3bc-a4d60ecbae02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
